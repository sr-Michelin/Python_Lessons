import numpy as np


def f(x):
    """Ф-ція активації у вигляді гіперболічного тангенса"""
    return 2 / (1 + np.exp(-x)) - 1


def df(x):
    """Похідна ф-ції активації"""
    return .5 * (1 + x) * (1 - x)


# початкові ваги для:
W1 = np.array([[-.2, .3, -.4], [.1, -.3, -.4]])  # першого шару
W2 = np.array([.2, .3])  # другого шару


def go_forward(inp):
    """Ф-ція пропуску вектору спостереження через нейронку"""
    sum = np.dot(W1, inp)
    out = np.array([f(x) for x in sum])  # вихідне значення для прихованого шару

    sum = np.dot(W2, out)  # вихідне значення для усієї нейронки
    y = f(sum)
    return (y, out)


def train(epoch):
    """Ф-ція навчання нейронної мережі"""
    global W1, W2
    lmd = .01  # точність навчання нейронки
    N = 10000  # кількість операцій
    count = len(epoch)

    for k in range(N):
        x = epoch[np.random.randint(0, count)]  # випадковий вибір вхідного сигналу для начання нейронки
        y, out = go_forward(x[0:3])  # прямий прохід по НМ та обчислення вхідних значень нейрону ****
        e = y - x[-1]  # обрахунок відхилення
        delta = e * df(y)  # локальнний градієнт

        W2[0] = W2[0] - lmd * delta * out[0]  # коректування ваги для першого зв\'язку
        W2[1] = W2[1] - lmd * delta * out[1]  # коректування ваги для другого зв\'язку

        delta2 = W2 * delta * df(out)  # вектор із 2-х локальних мінімумів

        # коректування звязків для першого шару
        W1[0, :] = W1[0, :] - np.array(x[0:3]) * delta2[0] * lmd
        W1[1, :] = W1[0, :] - np.array(x[0:3]) * delta2[1] * lmd


# Навчальна вибірка (вона же повна вибірка)
epoch = [(-1, -1, 0, 0),
         (-1, -1, 1, 1),
         (-1, 1, 1, 1),
         (1, -1, 0, 0),
         (1, -1, 1, 1)]

train(epoch)  # запуск начання НМ

print('Нейронна мережа на основі зворотнього розповсюдження:\n'.upper())

# перевірка отрманих результатів
for x in epoch:
    y, out = go_forward(x[0:3])
    print(f"Вихідне значення НМ {x} : {y} => {x[-1]}")
